{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8699816,"sourceType":"datasetVersion","datasetId":5217809},{"sourceId":8844147,"sourceType":"datasetVersion","datasetId":5323075},{"sourceId":8850154,"sourceType":"datasetVersion","datasetId":5327135},{"sourceId":8893637,"sourceType":"datasetVersion","datasetId":5348274},{"sourceId":8954276,"sourceType":"datasetVersion","datasetId":5388886},{"sourceId":9021440,"sourceType":"datasetVersion","datasetId":5436431},{"sourceId":9111014,"sourceType":"datasetVersion","datasetId":5499069},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T16:01:11.144572Z","iopub.execute_input":"2024-09-22T16:01:11.144953Z","iopub.status.idle":"2024-09-22T16:01:11.550572Z","shell.execute_reply.started":"2024-09-22T16:01:11.144922Z","shell.execute_reply":"2024-09-22T16:01:11.549666Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cleanddata1/datasetv3cleand.csv\n/kaggle/input/17k-v2/dataframe_17k_v2.csv\n/kaggle/input/testsetvrai/test.csv\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:01:17.695439Z","iopub.execute_input":"2024-09-22T16:01:17.695929Z","iopub.status.idle":"2024-09-22T16:03:24.731804Z","shell.execute_reply.started":"2024-09-22T16:01:17.695899Z","shell.execute_reply":"2024-09-22T16:03:24.730362Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:03:24.734365Z","iopub.execute_input":"2024-09-22T16:03:24.734664Z","iopub.status.idle":"2024-09-22T16:03:44.181289Z","shell.execute_reply.started":"2024-09-22T16:03:24.734636Z","shell.execute_reply":"2024-09-22T16:03:44.180521Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-09-22 16:03:33.208520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-22 16:03:33.208621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-22 16:03:33.377924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:03:44.182266Z","iopub.execute_input":"2024-09-22T16:03:44.182510Z","iopub.status.idle":"2024-09-22T16:03:45.015060Z","shell.execute_reply.started":"2024-09-22T16:03:44.182488Z","shell.execute_reply":"2024-09-22T16:03:45.014164Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pip install pyarrow","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:03:45.017018Z","iopub.execute_input":"2024-09-22T16:03:45.017581Z","iopub.status.idle":"2024-09-22T16:03:57.314030Z","shell.execute_reply.started":"2024-09-22T16:03:45.017555Z","shell.execute_reply":"2024-09-22T16:03:57.312752Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (16.1.0)\nRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\nprint(\"wandb\")\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune_Llama3_8B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:03:57.315761Z","iopub.execute_input":"2024-09-22T16:03:57.316157Z","iopub.status.idle":"2024-09-22T16:04:01.382286Z","shell.execute_reply.started":"2024-09-22T16:03:57.316120Z","shell.execute_reply":"2024-09-22T16:04:01.381344Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nwandb\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlaila-machkour\u001b[0m (\u001b[33mlaila-machkour-faculty-of-science-ben-msick\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240922_160359-i8z81wxo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B/runs/i8z81wxo' target=\"_blank\">fresh-spaceship-34</a></strong> to <a href='https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B' target=\"_blank\">https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B/runs/i8z81wxo' target=\"_blank\">https://wandb.ai/laila-machkour-faculty-of-science-ben-msick/Fine-tune_Llama3_8B/runs/i8z81wxo</a>"},"metadata":{}}]},{"cell_type":"code","source":"\nbase_model='meta-llama/Meta-Llama-3.1-8B-Instruct'\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:04:01.383625Z","iopub.execute_input":"2024-09-22T16:04:01.383939Z","iopub.status.idle":"2024-09-22T16:04:01.389472Z","shell.execute_reply.started":"2024-09-22T16:04:01.383912Z","shell.execute_reply":"2024-09-22T16:04:01.388241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ntorch_dtype = torch.float16\nattn_implementation = \"eager\"\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation,\n    torch_dtype = torch.float16\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:04:01.390601Z","iopub.execute_input":"2024-09-22T16:04:01.390887Z","iopub.status.idle":"2024-09-22T16:06:49.118648Z","shell.execute_reply.started":"2024-09-22T16:04:01.390865Z","shell.execute_reply":"2024-09-22T16:06:49.117649Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c082a78fc28341aa800384a38fcfe177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da058b8ab5f4383b5b3e864b16c12d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b0c827676c4f888d56be0e1f8f1a50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f584055cf23748038be8a1c6ff643348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4bf9c3671d4c6ebdd6378d12748196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae4aae2e34864f62b1e08080e4c2593d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add2a3e941634e7fb97fd7e8f0f3469c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d31cdec083444d29e932a5d6de3ee37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cd7e4b13ad485894ba2bc4345ad201"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:06:49.119966Z","iopub.execute_input":"2024-09-22T16:06:49.120247Z","iopub.status.idle":"2024-09-22T16:06:50.402585Z","shell.execute_reply.started":"2024-09-22T16:06:49.120222Z","shell.execute_reply":"2024-09-22T16:06:50.401635Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df2b4df9076741eda7d74cb759c10de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e943711222b40ba98811a63260e167e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ee580b09e54d82847681beb9338711"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n# LoRA config\npeft_config = LoraConfig(\n    r=24,\n    lora_alpha=24,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:06:50.404090Z","iopub.execute_input":"2024-09-22T16:06:50.404887Z","iopub.status.idle":"2024-09-22T16:06:51.530697Z","shell.execute_reply.started":"2024-09-22T16:06:50.404852Z","shell.execute_reply":"2024-09-22T16:06:51.529619Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nfrom datasets import Dataset\n\ndataset = pd.read_csv('/kaggle/input/17k-v2/dataframe_17k_v2.csv')\ndataset\ndataset.drop('Unnamed: 0', axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:17.210340Z","iopub.execute_input":"2024-09-22T16:07:17.211044Z","iopub.status.idle":"2024-09-22T16:07:17.606459Z","shell.execute_reply.started":"2024-09-22T16:07:17.211011Z","shell.execute_reply":"2024-09-22T16:07:17.605663Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#dataset.drop('Unnamed: 0', axis=1, inplace=True)\n    # Check for NaNs or non-string values\n    #print(df.info())\nprint(dataset.isna().sum())\ndef clean_text(text):\n    if isinstance(text, str):\n        return text.replace('\\n', ' ').replace('nan', ' ').strip()\n    return text\n\n    # Apply the cleaning function to both columns\ndataset['Prompt'] = dataset['Prompt'].apply(clean_text)\ndataset['Response'] = dataset['Response'].apply(clean_text)\ndataset=dataset.dropna()\ndataset=dataset.drop_duplicates() ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:23.339638Z","iopub.execute_input":"2024-09-22T16:07:23.340355Z","iopub.status.idle":"2024-09-22T16:07:23.446237Z","shell.execute_reply.started":"2024-09-22T16:07:23.340324Z","shell.execute_reply":"2024-09-22T16:07:23.445292Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Prompt       1\nResponse    27\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the format_chat_template function\ndef format_chat_template(row):\n    system_prompt=\"You are a competent and friendly chatbot specializing in tourism in Morocco. Your job is to guide tourists and answer their questions about Morocco. Provide a conversational response. If you don't know the answer, just say I don't know. Do not make up an answer. Always respond in the same language as the user's question.\"\n    row_json = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": row[\"Prompt\"]},\n        {\"role\": \"assistant\", \"content\": row[\"Response\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the function to the DataFrame\ndataset = dataset.apply(format_chat_template, axis=1)\n\n# Check the result\nprint(dataset['text'][1])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:27.480541Z","iopub.execute_input":"2024-09-22T16:07:27.481326Z","iopub.status.idle":"2024-09-22T16:07:38.949837Z","shell.execute_reply.started":"2024-09-22T16:07:27.481293Z","shell.execute_reply":"2024-09-22T16:07:38.948958Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a competent and friendly chatbot specializing in tourism in Morocco. Your job is to guide tourists and answer their questions about Morocco. Provide a conversational response. If you don't know the answer, just say I don't know. Do not make up an answer. Always respond in the same language as the user's question.<|im_end|>\n<|im_start|>user\nWhat is the cost of the activity 'Marrakech: Agafay Desert Quad & Camel Rides with Dinner Show'?<|im_end|>\n<|im_start|>assistant\nThe cost of the activity 'Marrakech: Agafay Desert Quad & Camel Rides with Dinner Show' is 399 per person.<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"t=dataset['text'][1]\naa=t.split(\"assistant\")[1]\nprint(\"aa\",aa)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:38.951418Z","iopub.execute_input":"2024-09-22T16:07:38.951764Z","iopub.status.idle":"2024-09-22T16:07:38.958604Z","shell.execute_reply.started":"2024-09-22T16:07:38.951738Z","shell.execute_reply":"2024-09-22T16:07:38.957701Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"aa \nThe cost of the activity 'Marrakech: Agafay Desert Quad & Camel Rides with Dinner Show' is 399 per person.<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(dataset, test_size=0.015, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:44.324215Z","iopub.execute_input":"2024-09-22T16:07:44.324577Z","iopub.status.idle":"2024-09-22T16:07:44.336764Z","shell.execute_reply.started":"2024-09-22T16:07:44.324548Z","shell.execute_reply":"2024-09-22T16:07:44.335988Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:48.199718Z","iopub.execute_input":"2024-09-22T16:07:48.200869Z","iopub.status.idle":"2024-09-22T16:07:48.217370Z","shell.execute_reply.started":"2024-09-22T16:07:48.200834Z","shell.execute_reply":"2024-09-22T16:07:48.216512Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                                  Prompt  \\\n13719                              travel agency in Taza   \n3157   What should I bring for the activity 'Marrakec...   \n12242                                         VIP TRAVEL   \n13686                                       UNION TRAVEL   \n720    What are the highlights of the activity 'From ...   \n...                                                  ...   \n4115   hotel facilities: 2 swimming pools, Spa, Airpo...   \n8306                          Casa de los Dragones Ceuta   \n2485   What is not included in the activity 'Casablan...   \n3      What does the activity 'Marrakech: Agafay Dese...   \n11671                        travel agency in Casablanca   \n\n                                                Response  \\\n13719  STE CONTINENT VOYAGES Adresse: Av. Allal El Fa...   \n3157   For the activity 'Marrakech: Jemma El Fnaa Foo...   \n12242  VIP TRAVEL Adresse: 217, Bd Hachmi Filali, Lot...   \n13686  UNION TRAVEL Adresse: 2 Bis rue El Antaki Tel:...   \n720    The highlights of the activity 'From Marrakech...   \n...                                                  ...   \n4115   Hyatt Place Taghazout Bay in Taghazout. The ho...   \n8306   The Casa de los Dragones on Plaza de los Reyes...   \n2485   The activity 'Casablanca: Private Guided Tour ...   \n3      The activity 'Marrakech: Agafay Desert Quad & ...   \n11671  ALIZES TRAVEL Adresse: 166-168 Avenue Mers Sul...   \n\n                                                    text  \n13719  <|im_start|>system\\nYou are a competent and fr...  \n3157   <|im_start|>system\\nYou are a competent and fr...  \n12242  <|im_start|>system\\nYou are a competent and fr...  \n13686  <|im_start|>system\\nYou are a competent and fr...  \n720    <|im_start|>system\\nYou are a competent and fr...  \n...                                                  ...  \n4115   <|im_start|>system\\nYou are a competent and fr...  \n8306   <|im_start|>system\\nYou are a competent and fr...  \n2485   <|im_start|>system\\nYou are a competent and fr...  \n3      <|im_start|>system\\nYou are a competent and fr...  \n11671  <|im_start|>system\\nYou are a competent and fr...  \n\n[256 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>Response</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13719</th>\n      <td>travel agency in Taza</td>\n      <td>STE CONTINENT VOYAGES Adresse: Av. Allal El Fa...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>3157</th>\n      <td>What should I bring for the activity 'Marrakec...</td>\n      <td>For the activity 'Marrakech: Jemma El Fnaa Foo...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>12242</th>\n      <td>VIP TRAVEL</td>\n      <td>VIP TRAVEL Adresse: 217, Bd Hachmi Filali, Lot...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>13686</th>\n      <td>UNION TRAVEL</td>\n      <td>UNION TRAVEL Adresse: 2 Bis rue El Antaki Tel:...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>720</th>\n      <td>What are the highlights of the activity 'From ...</td>\n      <td>The highlights of the activity 'From Marrakech...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4115</th>\n      <td>hotel facilities: 2 swimming pools, Spa, Airpo...</td>\n      <td>Hyatt Place Taghazout Bay in Taghazout. The ho...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>8306</th>\n      <td>Casa de los Dragones Ceuta</td>\n      <td>The Casa de los Dragones on Plaza de los Reyes...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>2485</th>\n      <td>What is not included in the activity 'Casablan...</td>\n      <td>The activity 'Casablanca: Private Guided Tour ...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What does the activity 'Marrakech: Agafay Dese...</td>\n      <td>The activity 'Marrakech: Agafay Desert Quad &amp; ...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n    <tr>\n      <th>11671</th>\n      <td>travel agency in Casablanca</td>\n      <td>ALIZES TRAVEL Adresse: 166-168 Avenue Mers Sul...</td>\n      <td>&lt;|im_start|&gt;system\\nYou are a competent and fr...</td>\n    </tr>\n  </tbody>\n</table>\n<p>256 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    save_steps=0,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n) \n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:51.624869Z","iopub.execute_input":"2024-09-22T16:07:51.625464Z","iopub.status.idle":"2024-09-22T16:07:51.663675Z","shell.execute_reply.started":"2024-09-22T16:07:51.625436Z","shell.execute_reply":"2024-09-22T16:07:51.662800Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert pandas DataFrame to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:54.259876Z","iopub.execute_input":"2024-09-22T16:07:54.260264Z","iopub.status.idle":"2024-09-22T16:07:54.613695Z","shell.execute_reply.started":"2024-09-22T16:07:54.260235Z","shell.execute_reply":"2024-09-22T16:07:54.612725Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:56.569383Z","iopub.execute_input":"2024-09-22T16:07:56.569759Z","iopub.status.idle":"2024-09-22T16:07:56.577832Z","shell.execute_reply.started":"2024-09-22T16:07:56.569730Z","shell.execute_reply":"2024-09-22T16:07:56.576946Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Prompt', 'Response', 'text', '__index_level_0__'],\n    num_rows: 16804\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Define SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:07:59.884442Z","iopub.execute_input":"2024-09-22T16:07:59.884863Z","iopub.status.idle":"2024-09-22T16:08:07.217609Z","shell.execute_reply.started":"2024-09-22T16:07:59.884827Z","shell.execute_reply":"2024-09-22T16:08:07.216562Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16804 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a7405fb9a94045821d3fc195261406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/256 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ca83c5c4cd4bc2bea20d45e7a4ce69"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:08:07.219849Z","iopub.execute_input":"2024-09-22T16:08:07.220256Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2572' max='8402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2572/8402 1:33:51 < 3:32:55, 0.46 it/s, Epoch 0.31/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1681</td>\n      <td>0.700700</td>\n      <td>0.839731</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"bigllama3\")\ntokenizer.save_pretrained(\"bigllama3\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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')\nfrom transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(model_name)\nconfig.push_to_hub(\"laila1234/tourllama3.1\")\ntrainer.model.config.push_to_hub(\"laila1234/tourllama3.1\")\ntokenizer.push_to_hub(\"laila1234/tourllama3.1\")\nmodel.push_to_hub(\"laila1234/tourllama3.1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"best laces to visit in rabat\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=400, \n                         num_return_sequences=1,\n    num_beams=3,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.8 )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=5,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.8 ,early_stopping=True)\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n)\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=7,\n   )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=7,\n    temperature=1,\n    )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=7,\n    temperature=1,repetition_penalty=1.8,\n    )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=7,\n    temperature=0.5,repetition_penalty=1.8\n    )\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the top tourist attractions in Marrakech?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=350, \n                         num_return_sequences=1,\n    num_beams=5,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.8 ,early_stopping=True)\n\n\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text.split(\"assistant\")[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\n# Charger le dataset\ndf = pd.read_csv('/kaggle/input/testsetvrai/test.csv')\nprint(len(df)) \n\n\n\n\ndef generate_responses(df,i):\n    \n    responses = []\n    \n    for p in df['prompt']:\n        messages = [{\n        \"role\": \"user\",\n        \"content\": p }]\n\n        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n        outputs = model.generate(\n    **inputs,\n    max_length=350, \n                         num_return_sequences=1,\n    num_beams=5,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.8 ,early_stopping=True# Ajouter une pénalité de répétition\n)\n\n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\n        print(i)\n        \n        responses.append(text.split(\"assistant\")[1])\n        i=i+1\n    df['generated_response'] = responses\n    \n    return df\n\ndf = generate_responses(df,1)\ndf.to_csv('resultats_chatbot_bigllama3_18k-finetuning.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Download NLTK resources\nnltk.download('punkt')\n\n# Initialize the GPT-2 model and tokenizer\nmodel_name = 'gpt2'\nmodelgpt = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizergpt = GPT2Tokenizer.from_pretrained(model_name)\n\n# Initialize the ROUGE scorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Function to calculate perplexity\ndef calculate_perplexity(text):\n    inputs = tokenizergpt(text, return_tensors='pt', truncation=True, max_length=1024)\n    outputs = modelgpt(**inputs, labels=inputs[\"input_ids\"])\n    loss = outputs.loss\n    return torch.exp(loss).item()\n\n# Function to calculate BLEU, ROUGE, and perplexity scores\ndef calculate_scores(df):\n    bleu_scores = []\n    rouge_scores_1 = []\n    rouge_scores_2 = []\n    rouge_scores_l = []\n    perplexities = []\n\n    for idx, row in df.iterrows():\n        context = row['answer']\n        response = row['generated_response']\n\n        # BLEU score with smoothing function\n        context_tokens = nltk.word_tokenize(context)\n        response_tokens = nltk.word_tokenize(response)\n        bleu = sentence_bleu([context_tokens], response_tokens, smoothing_function=SmoothingFunction().method1)\n        bleu_scores.append(bleu)\n\n        # ROUGE scores\n        rouge_score = scorer.score(context, response)\n        rouge_scores_1.append(rouge_score['rouge1'].fmeasure)\n        rouge_scores_2.append(rouge_score['rouge2'].fmeasure)\n        rouge_scores_l.append(rouge_score['rougeL'].fmeasure)\n\n        # Perplexity\n        perplexity = calculate_perplexity(response)\n        perplexities.append(perplexity)\n\n    # Add scores to DataFrame\n    df['BLEU'] = bleu_scores\n    df['ROUGE-1'] = rouge_scores_1\n    df['ROUGE-2'] = rouge_scores_2\n    df['ROUGE-L'] = rouge_scores_l\n    df['Perplexity'] = perplexities\n\n    return df\n\n# Example DataFrame with questions and human-written answers\n\n\n# Calculate scores\ndf = calculate_scores(df)\n\n# Calculate global scores\nmean_bleu = df['BLEU'].mean()\nmean_rouge_1 = df['ROUGE-1'].mean()\nmean_rouge_2 = df['ROUGE-2'].mean()\nmean_rouge_l = df['ROUGE-L'].mean()\nmean_perplexity = df['Perplexity'].mean()\n\nprint(\"Scores globaux:\")\nprint(f\"BLEU: {mean_bleu}\")\nprint(f\"ROUGE-1: {mean_rouge_1}\")\nprint(f\"ROUGE-2: {mean_rouge_2}\")\nprint(f\"ROUGE-L: {mean_rouge_l}\")\nprint(f\"Perplexity: {mean_perplexity}\")\n\n# Save results to CSV\ndf.to_csv('evaluation_results_fine_tuning1_llama3.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
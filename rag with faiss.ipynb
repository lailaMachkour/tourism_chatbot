{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8765200,
          "sourceType": "datasetVersion",
          "datasetId": 5266705
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpFbLd3mlfZA",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:11:27.272388Z",
          "iopub.execute_input": "2024-06-23T15:11:27.272701Z",
          "iopub.status.idle": "2024-06-23T15:11:28.266566Z",
          "shell.execute_reply.started": "2024-06-23T15:11:27.272662Z",
          "shell.execute_reply": "2024-06-23T15:11:28.265568Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 bitsandbytes==0.40.2 transformers sentence-transformers"
      ],
      "metadata": {
        "id": "38FjaLDKt9JD",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:11:28.268353Z",
          "iopub.execute_input": "2024-06-23T15:11:28.268766Z",
          "iopub.status.idle": "2024-06-23T15:11:46.004418Z",
          "shell.execute_reply.started": "2024-06-23T15:11:28.268713Z",
          "shell.execute_reply": "2024-06-23T15:11:46.003109Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6365932-987c-4920-97ef-887c26b751ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets faiss-cpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T15:11:46.006120Z",
          "iopub.execute_input": "2024-06-23T15:11:46.006867Z",
          "iopub.status.idle": "2024-06-23T15:12:00.054001Z",
          "shell.execute_reply.started": "2024-06-23T15:11:46.006826Z",
          "shell.execute_reply": "2024-06-23T15:12:00.052884Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlhMpcArxmd_",
        "outputId": "5905dec8-e302-49bd-a99d-1f6112666bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, faiss-cpu, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 faiss-cpu-1.8.0.post1 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "\n",
        "    pipeline,\n",
        "    logging,\n",
        ")"
      ],
      "metadata": {
        "id": "NjD9rmAMnpz0",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:12:00.056515Z",
          "iopub.execute_input": "2024-06-23T15:12:00.056845Z",
          "iopub.status.idle": "2024-06-23T15:12:17.470006Z",
          "shell.execute_reply.started": "2024-06-23T15:12:00.056816Z",
          "shell.execute_reply": "2024-06-23T15:12:17.469225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Se connecter à Hugging Face avec le token\n",
        "login(token=\"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\")\n",
        "use_4bit = True\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "device_map = {\"\": 0}\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "model_id= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# Load base model\n",
        "print(\"model\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T15:12:17.471144Z",
          "iopub.execute_input": "2024-06-23T15:12:17.471480Z",
          "iopub.status.idle": "2024-06-23T15:17:09.507509Z",
          "shell.execute_reply.started": "2024-06-23T15:12:17.471448Z",
          "shell.execute_reply": "2024-06-23T15:17:09.506551Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "u7Rfy2X7xmd_",
        "outputId": "5e4a4043-0198-45b4-e1c7-f0df76eead73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ac5863ba99f4>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Load configuration and manually adjust 'rope_scaling'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrope_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"factor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrope_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"factor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Adjust 'factor' as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Initialisez le modèle SentenceTransformer\n",
        "ST = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Définissez la fonction pour générer les embeddings\n",
        "def embed(batch):\n",
        "    information = batch[\"Response\"]\n",
        "    return {\"embeddings\": ST.encode(information)}\n",
        "# Charger votre ensemble de données CSV\n",
        "df = pd.read_csv(\"/content/tourism_data.csv\")\n",
        "\n",
        "# Initialiser le modèle SentenceTransformer\n",
        "\n",
        "# Convertissez les données en un objet Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "# Appliquez la fonction d'intégration aux données\n",
        "dataset = dataset.map(embed, batched=True, batch_size=16)\n",
        "\n",
        "# Si vous souhaitez indexer les embeddings avec FAISS\n",
        "# data = dataset.add_faiss_index(\"embeddings\")\n",
        "\n",
        "# Maintenant, vous pouvez accéder à vos données avec les embeddings\n",
        "print(dataset)\n",
        "data = dataset.add_faiss_index(\"embeddings\") # column name that has the embeddings of the dataset\n",
        "\n",
        "def search(query: str ):\n",
        "\n",
        "    \"\"\"a function that embeds a new query and returns the most probable results\"\"\"\n",
        "    embedded_query = ST.encode(query) # embed new query\n",
        "    scores, retrieved_examples = data.get_nearest_examples( # retrieve results\n",
        "        \"embeddings\", embedded_query, # compare our new embedded query with the dataset embeddings\n",
        "        k=3 # get only top k results\n",
        "    )\n",
        "    return scores, retrieved_examples\n",
        "# Utilisez dataset[\"train\"] pour accéder aux données d'entraînement, si nécessaire\n"
      ],
      "metadata": {
        "id": "yEJYDtMDqMyd",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:34:23.345058Z",
          "iopub.execute_input": "2024-06-23T15:34:23.345847Z",
          "iopub.status.idle": "2024-06-23T15:35:39.069326Z",
          "shell.execute_reply.started": "2024-06-23T15:34:23.345814Z",
          "shell.execute_reply": "2024-06-23T15:35:39.068255Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T15:26:14.167648Z",
          "iopub.execute_input": "2024-06-23T15:26:14.167957Z",
          "iopub.status.idle": "2024-06-23T15:26:29.824157Z",
          "shell.execute_reply.started": "2024-06-23T15:26:14.167931Z",
          "shell.execute_reply": "2024-06-23T15:26:29.823170Z"
        },
        "trusted": true,
        "id": "Tic9VPvSxmeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKpxbWnrAfcD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYS_PROMPT =  \"\"\"Vous êtes un chatbot compétent et amical spécialisé dans le tourisme au Maroc. Votre rôle est de guider les touristes et de répondre à leurs questions sur le Maroc. Fournissez une réponse conversationnelle. Si vous ne connaissez pas la réponse, dites simplement \"Je ne sais pas\". Ne fabriquez pas de réponse. La réponse doit être dans la même langue que la langue de la question posée par l'utilisateur.\"\"\"\n",
        "\n",
        "\n",
        "def format_prompt(prompt,retrieved_documents,k):\n",
        "\n",
        "    PROMPT = f\"Question:{prompt}\\nContext:\"\n",
        "\n",
        "    for idx in range(k) :\n",
        "        PROMPT+= f\"{retrieved_documents['Response'][idx]}\\n\"\n",
        "    return PROMPT\n",
        "\n",
        "def generate(formatted_prompt):\n",
        "\n",
        "    formatted_prompt = formatted_prompt[:2000] # to avoid GPU OOM\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":formatted_prompt}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "      ).to(model.device)\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.4,\n",
        "\n",
        "  )\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def rag_chatbot(prompt:str,k:int=2):\n",
        "\n",
        "\n",
        "    scores, retrieved_documents = search(prompt)\n",
        "    print(\"scores\", scores)\n",
        "    print(\"retrieved_documents\", retrieved_documents)\n",
        "    print()\n",
        "\n",
        "    formatted_prompt = format_prompt(prompt, retrieved_documents, k)\n",
        "    print()\n",
        "    print(\"formatted_prompt\", formatted_prompt)\n",
        "    print()\n",
        "    response = generate(formatted_prompt)\n",
        "    print(response)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSM93R0KpcYq",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:38:01.079030Z",
          "iopub.execute_input": "2024-06-23T15:38:01.079953Z",
          "iopub.status.idle": "2024-06-23T15:38:01.090029Z",
          "shell.execute_reply.started": "2024-06-23T15:38:01.079921Z",
          "shell.execute_reply": "2024-06-23T15:38:01.089041Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chatbot('\"What are the top tourist attractions in Marrakech?')"
      ],
      "metadata": {
        "id": "xoF2DjOmu9Kh",
        "execution": {
          "iopub.status.busy": "2024-06-23T15:42:19.163782Z",
          "iopub.execute_input": "2024-06-23T15:42:19.164732Z",
          "iopub.status.idle": "2024-06-23T15:42:19.678521Z",
          "shell.execute_reply.started": "2024-06-23T15:42:19.164675Z",
          "shell.execute_reply": "2024-06-23T15:42:19.676978Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "-s4_PLewxmeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "qp3PElKB19eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "UV0Ly8aV19f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Generated Responses'] = \"\"\n",
        "# Function to simulate response generation (replace with your actual function)\n",
        "def rag_chatbot(prompt):\n",
        "    # Simulate generating a response\n",
        "    return f\"Generated response for: {prompt}\"\n",
        "# Generate responses for each prompt and update the DataFrame\n",
        "for idx, row in df.iterrows():\n",
        "    prompt = row['prompt']\n",
        "    response = rag_chatbot(prompt)\n",
        "    df.at[idx, 'Generated Responses'] = response"
      ],
      "metadata": {
        "id": "RJRnWKgj19iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chatbot(\"What are the top tourist attractions in Marrakech?\")"
      ],
      "metadata": {
        "id": "17CoWOYmGePc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "NsNAkSC3B3mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "modelgpt = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizergpt = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate perplexity\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tokenizergpt(text, return_tensors='pt', truncation=True, max_length=1024)\n",
        "    outputs = modelgpt(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    return torch.exp(loss).item()\n",
        "\n",
        "# Function to calculate BLEU, ROUGE, and perplexity scores\n",
        "def calculate_scores(df):\n",
        "    bleu_scores = []\n",
        "    rouge_scores_1 = []\n",
        "    rouge_scores_2 = []\n",
        "    rouge_scores_l = []\n",
        "    perplexities = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        context = row['answer']\n",
        "        response = row['Generated Responses']\n",
        "\n",
        "        # BLEU score with smoothing function\n",
        "        context_tokens = nltk.word_tokenize(context)\n",
        "        response_tokens = nltk.word_tokenize(response)\n",
        "        bleu = sentence_bleu([context_tokens], response_tokens, smoothing_function=SmoothingFunction().method1)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE scores\n",
        "        rouge_score = scorer.score(context, response)\n",
        "        rouge_scores_1.append(rouge_score['rouge1'].fmeasure)\n",
        "        rouge_scores_2.append(rouge_score['rouge2'].fmeasure)\n",
        "        rouge_scores_l.append(rouge_score['rougeL'].fmeasure)\n",
        "\n",
        "        # Perplexity\n",
        "        perplexity = calculate_perplexity(response)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "    # Add scores to DataFrame\n",
        "    df['BLEU'] = bleu_scores\n",
        "    df['ROUGE-1'] = rouge_scores_1\n",
        "    df['ROUGE-2'] = rouge_scores_2\n",
        "    df['ROUGE-L'] = rouge_scores_l\n",
        "    df['Perplexity'] = perplexities\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example DataFrame with questions and human-written answers\n",
        "\n",
        "\n",
        "# Calculate scores\n",
        "df = calculate_scores(df)\n",
        "\n",
        "# Calculate global scores\n",
        "mean_bleu = df['BLEU'].mean()\n",
        "mean_rouge_1 = df['ROUGE-1'].mean()\n",
        "mean_rouge_2 = df['ROUGE-2'].mean()\n",
        "mean_rouge_l = df['ROUGE-L'].mean()\n",
        "mean_perplexity = df['Perplexity'].mean()\n",
        "\n",
        "print(\"Scores globaux:\")\n",
        "print(f\"BLEU: {mean_bleu}\")\n",
        "print(f\"ROUGE-1: {mean_rouge_1}\")\n",
        "print(f\"ROUGE-2: {mean_rouge_2}\")\n",
        "print(f\"ROUGE-L: {mean_rouge_l}\")\n",
        "print(f\"Perplexity: {mean_perplexity}\")\n",
        "\n",
        "# Save results to CSV\n",
        "df.to_csv('evaluation_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "3d10QqTuDkNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score transformers\n"
      ],
      "metadata": {
        "id": "M1hPATCbDxmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer # Import the rouge_scorer module\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True) # Initialize the rouge scorer\n",
        "\n",
        "# Initialiser le modèle et tokenizer GPT-2 pour calculer la perplexité\n",
        "model_name = 'gpt2'\n",
        "modelgpt = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizergpt = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tokenizergpt(text, return_tensors='pt', truncation=True, max_length=1024)\n",
        "    outputs = modelgpt(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    return torch.exp(loss).item()\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "# Calculer les scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "def calculate_scores(df):\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    meteor_scores = []\n",
        "    perplexities = []\n",
        "\n",
        "    for context, response in zip(df['answer'], df['generated_response']):\n",
        "        # Tokenizer le context et la réponse\n",
        "        context_tokens = nltk.word_tokenize(context)\n",
        "        response_tokens = nltk.word_tokenize(response)\n",
        "\n",
        "        # BLEU score avec fonction de lissage\n",
        "        bleu = sentence_bleu([context_tokens], response_tokens, smoothing_function=smoothing_function)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE score\n",
        "        rouge_score = scorer.score(context, response)\n",
        "        rouge_scores.append(rouge_score)\n",
        "\n",
        "\n",
        "\n",
        "        # Perplexity\n",
        "        perplexity = calculate_perplexity(response)\n",
        "        perplexities.append(perplexity)\n",
        "\n",
        "    df['bleu'] = bleu_scores\n",
        "    df['rouge'] = rouge_scores\n",
        "\n",
        "    df['perplexity'] = perplexities\n",
        "\n",
        "    return df\n",
        "\n",
        "df = calculate_scores(df)"
      ],
      "metadata": {
        "id": "QBDechXl19lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder les résultats dans un nouveau fichier CSV localement\n",
        "df.to_csv('eval_chatbot_fr1_ft_2.csv')\n",
        "\n",
        "# Calculer les scores globaux\n",
        "def calculate_global_scores(df):\n",
        "    mean_bleu = df['bleu'].mean()\n",
        "    mean_rouge = {\n",
        "        'rouge-1': sum([score['rouge1'].fmeasure for score in df['rouge']]) / len(df),\n",
        "        'rouge-2': sum([score['rouge2'].fmeasure for score in df['rouge']]) / len(df),\n",
        "        'rouge-l': sum([score['rougeL'].fmeasure for score in df['rouge']]) / len(df)\n",
        "    }\n",
        "\n",
        "    mean_perplexity = df['perplexity'].mean()\n",
        "\n",
        "    return {\n",
        "        'mean_bleu': mean_bleu,\n",
        "        'mean_rouge': mean_rouge,\n",
        "\n",
        "        'mean_perplexity': mean_perplexity\n",
        "    }\n",
        "\n",
        "global_scores = calculate_global_scores(df)\n",
        "\n",
        "print(\"Scores globaux:\")\n",
        "print(f\"BLEU: {global_scores['mean_bleu']}\")\n",
        "print(f\"ROUGE-1: {global_scores['mean_rouge']['rouge-1']}\")\n",
        "print(f\"ROUGE-2: {global_scores['mean_rouge']['rouge-2']}\")\n",
        "print(f\"ROUGE-L: {global_scores['mean_rouge']['rouge-l']}\")\n",
        "\n",
        "print(f\"Perplexity: {global_scores['mean_perplexity']}\")"
      ],
      "metadata": {
        "id": "dWZmND725DOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(token=\"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\")\n",
        "\n",
        "use_4bit = True\n",
        "compute_dtype = getattr(torch, \"bfloat16\")\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Check GPU compatibility\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Load base model with BitsAndBytesConfig\n",
        "print(\"Loading model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=compute_dtype,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"\")\n",
        "]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Initialize SentenceTransformer model\n",
        "ST = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Define embedding function\n",
        "def embed(batch):\n",
        "    information = batch[\"Response\"]\n",
        "    return {\"embeddings\": ST.encode(information)}\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/ragllama3/datasetv3cleand.csv\")\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(embed, batched=True, batch_size=16)\n",
        "data = dataset.add_faiss_index(\"embeddings\")\n",
        "\n",
        "def search(query: str):\n",
        "    \"\"\"Function to embed a new query and return the most probable results\"\"\"\n",
        "    embedded_query = ST.encode(query)\n",
        "    scores, retrieved_examples = data.get_nearest_examples(\"embeddings\", embedded_query, k=3)\n",
        "    return scores, retrieved_examples\n",
        "\n",
        "SYS_PROMPT = \"\"\"You are a competent and friendly chatbot specializing in tourism in Morocco. Your job is to guide tourists and answer their questions about Morocco. Provide a conversational response. If you don't know the answer, just say I don't know. Do not make up an answer. The answer must be in the same language as the language of the question asked by the user.\"\"\"\n",
        "\n",
        "def format_prompt(prompt, retrieved_documents, k):\n",
        "    PROMPT = f\"Question:{prompt}\\nContext:\"\n",
        "    for idx in range(k):\n",
        "        PROMPT += f\"{retrieved_documents['Response'][idx]}\\n\"\n",
        "    return PROMPT\n",
        "\n",
        "def generate(formatted_prompt):\n",
        "    formatted_prompt = formatted_prompt[:2000]\n",
        "    input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=terminators[0],\n",
        "        do_sample=True,\n",
        "        temperature=0.4\n",
        "    )\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def rag_chatbot(prompt: str, k: int = 2):\n",
        "    scores, retrieved_documents = search(prompt)\n",
        "    formatted_prompt = format_prompt(prompt, retrieved_documents, k)\n",
        "    response = generate(formatted_prompt)\n",
        "    return response\n",
        "\n",
        "print(rag_chatbot(\"Quelle est la meilleure période pour visiter le Maroc ?\"))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T16:13:02.454326Z",
          "iopub.execute_input": "2024-06-23T16:13:02.455161Z",
          "iopub.status.idle": "2024-06-23T16:14:12.907207Z",
          "shell.execute_reply.started": "2024-06-23T16:13:02.455126Z",
          "shell.execute_reply": "2024-06-23T16:14:12.906014Z"
        },
        "trusted": true,
        "id": "dZZs4oEYxmeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch transformers\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T16:01:10.745375Z",
          "iopub.execute_input": "2024-06-23T16:01:10.745815Z",
          "iopub.status.idle": "2024-06-23T16:03:29.768226Z",
          "shell.execute_reply.started": "2024-06-23T16:01:10.745783Z",
          "shell.execute_reply": "2024-06-23T16:03:29.766888Z"
        },
        "trusted": true,
        "id": "XtVJmmoNxmeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-23T16:03:29.770846Z",
          "iopub.execute_input": "2024-06-23T16:03:29.771327Z",
          "iopub.status.idle": "2024-06-23T16:03:29.777420Z",
          "shell.execute_reply.started": "2024-06-23T16:03:29.771278Z",
          "shell.execute_reply": "2024-06-23T16:03:29.776340Z"
        },
        "trusted": true,
        "id": "KwBoCIjExmeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WRUce5LxmeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}